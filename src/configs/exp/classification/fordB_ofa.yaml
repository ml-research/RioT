seed_everything: 34234
trainer:
  max_epochs: 50

model: 
  __base__: ofa.yaml
  init_args:
    max_seq_len: 500 
    num_classes: 2
    feat_dim: 1
    patch_size: 8
    stride: 8
    dropout: 0.1

data:
  __base__: fordB.yaml
  init_args:
    batch_size: 64 
    lambda_time: 100
    lambda_freq: 10
    scaler:
      class_path: lib.preprocessing.StandardScaler

    
optimizer:
  # class_path: torch.optim.RAdam
  lr: 5e-3
# classification_explanation_plot:
#   explanation_method: INTEGRATED_GRADIENTS
#   log_interval: 5
#   freq_lim: 0.5
#ckpt_path: # ../checkpoints/classification/fordB/FCN/9eb3d582e4f4444096acca1f/epoch=39-step=9999.ckpt